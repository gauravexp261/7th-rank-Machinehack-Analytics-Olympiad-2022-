{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "653eb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer,MissingIndicator, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_validate,GridSearchCV,KFold,StratifiedKFold,StratifiedGroupKFold,cross_val_score,cross_val_predict,KFold\n",
    "from sklearn.preprocessing import PowerTransformer,OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import make_column_transformer,make_column_selector,ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, GradientBoostingRegressor,RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import  KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "from feature_engine.selection import SelectByShuffling,DropHighPSIFeatures,SelectByInformationValue\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import feature_engine\n",
    "from sklearn.preprocessing import KBinsDiscretizer,LabelEncoder\n",
    "from feature_engine.encoding import OrdinalEncoder,PRatioEncoder\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "from feature_engine.encoding import RareLabelEncoder,DecisionTreeEncoder\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures\n",
    "from feature_engine.selection import (RecursiveFeatureElimination, RecursiveFeatureAddition,\n",
    "    DropConstantFeatures,\n",
    "    DropDuplicateFeatures,\n",
    "    SmartCorrelatedSelection,DropCorrelatedFeatures\n",
    ")\n",
    "from feature_engine.creation import MathFeatures,RelativeFeatures\n",
    "from scipy.stats import chi2_contingency\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.encoding import StringSimilarityEncoder\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.encoding import MeanEncoder\n",
    "from sklearn import metrics\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',50)\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f025069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6fed1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping postal codes as these belongs to different countries and dont have any relevance \n",
    "y=train['OUTCOME']\n",
    "data=pd.concat((train,test)).reset_index(drop=True)\n",
    "data=data.drop(columns=['ID','OUTCOME','POSTAL_CODE'],axis=1)  \n",
    "\n",
    "train=data[:len(train)]\n",
    "test=data[len(train):]\n",
    "train['target']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4678,
   "id": "0e9b2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combing rare values which are present less than 1 percent in data \n",
    "def rare_label(df):    \n",
    "    df['DUIS']=df['DUIS'].apply(lambda x: -999 if x>=2 else x)\n",
    "    df['ANNUAL_MILEAGE']=df['ANNUAL_MILEAGE'].apply(lambda x: -999 if x in [5000.0,17000.0,2000.0,18000.0,19000.0,20000.0,21000.0] else x)\n",
    "    df['SPEEDING_VIOLATIONS']=df['SPEEDING_VIOLATIONS'].apply(lambda x: -999 if x>=5 else x)\n",
    "    df['PAST_ACCIDENTS']=df['PAST_ACCIDENTS'].apply(lambda x: -999 if x>=5 else x)\n",
    "#    df['POSTAL_CODE']=df['POSTAL_CODE'].astype('object')\n",
    "    return df\n",
    "train=rare_label(train)\n",
    "test=rare_label(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4344,
   "id": "58202591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def fe1(df):\n",
    "#     df['total_violations']=df['SPEEDING_VIOLATIONS']+df['DUIS']+df['PAST_ACCIDENTS']\n",
    "#     df['family_members']=df['MARRIED']*2 + df['CHILDREN']\n",
    "   \n",
    "  \n",
    "    \n",
    "#     df['CREDIT_SCORE']=(df['CREDIT_SCORE']*1000).round().astype('int64')\n",
    "#     df['CREDIT_SCORE_rating']=df['CREDIT_SCORE'].apply(lambda x: 3 if x>=720 else 2 if x>=690 else 1 if x>=630 else 0)\n",
    "    \n",
    "# #     df['PAST_ACCIDENTS_bi']=df['PAST_ACCIDENTS'].apply(lambda x: 1 if x==0 else 0)\n",
    "# #     df['DUIS_bi']=df['DUIS'].apply(lambda x: 1 if x==0 else 0)\n",
    "# #     df['SPEEDING_VIOLATIONS_bi']=df['SPEEDING_VIOLATIONS'].apply(lambda x: 1 if x==0 else 0)\n",
    "# #     df['ANNUAL_MILEAGE_rating']=df['ANNUAL_MILEAGE'].apply(lambda x: 4 if x>12000 else 3 if x> 10000 else 2 if x>8000 else 1 if x> 5000 else 0)\n",
    "    \n",
    "#     df['credit_cat']=df['CREDIT_SCORE'].apply(lambda x: 3 if x>700 else 2 if x>600 else 1 if x>515 else 0)\n",
    "#     df['ANNUAL_MILEAGE']=df['ANNUAL_MILEAGE'].apply(lambda x: 0 if x>13000 else 1 if x>11000 else 2 if x>9000 else 3 )\n",
    "    \n",
    "#     del df['SPEEDING_VIOLATIONS'],df['DUIS'],df['PAST_ACCIDENTS']\n",
    "    \n",
    "   \n",
    "#     return df\n",
    "# #data=fe1(data)\n",
    "# # test=fe1(test)\n",
    "# train=fe1(train)\n",
    "# test=fe1(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3135,
   "id": "df6d54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['c0']=data['CHILDREN']*data['VEHICLE_OWNERSHIP']\n",
    "\n",
    "# data['c1']=data.groupby(['TYPE_OF_VEHICLE'])['CHILDREN'].transform('max')\n",
    "# data['c2']=data.groupby(['TYPE_OF_VEHICLE'])['CHILDREN'].transform('mean')\n",
    "# data['c3']=data.groupby(['TYPE_OF_VEHICLE'])['CHILDREN'].transform('min')\n",
    "# data['c4']=data.groupby(['TYPE_OF_VEHICLE'])['CHILDREN'].transform('std')\n",
    "\n",
    "# data['c5']=data.groupby(['TYPE_OF_VEHICLE'])['VEHICLE_OWNERSHIP'].transform('max')\n",
    "# data['c6']=data.groupby(['TYPE_OF_VEHICLE'])['VEHICLE_OWNERSHIP'].transform('mean')\n",
    "# data['c7']=data.groupby(['TYPE_OF_VEHICLE'])['VEHICLE_OWNERSHIP'].transform('min')\n",
    "# data['c8']=data.groupby(['TYPE_OF_VEHICLE'])['VEHICLE_OWNERSHIP'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2376861",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3136,
   "id": "a25c4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr(df):    \n",
    "    df['VEHICLE_YEAR_INCOME']=df['VEHICLE_YEAR'].astype(\"str\")+'_'+df['INCOME'].astype(\"str\")\n",
    "  #  df['POSTAL_CODE'].astype(\"str\")+'_'+df['AGE'].astype(\"str\")\n",
    "    return df\n",
    "train=gr(train)\n",
    "test=gr(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4679,
   "id": "6640b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(df_train,df_test):\n",
    "    \n",
    "    cats=['TYPE_OF_VEHICLE','DRIVING_EXPERIENCE']\n",
    "    f1 = df_train[cats + ['target']].groupby(cats).agg(['mean', 'count']).reset_index()\n",
    "    f1.columns = cats +['f1'+' '.join(col).strip() for col in f1.columns.values][2:]\n",
    "    df_train=df_train.merge(f1,on=cats,how='left')\n",
    "    df_test=df_test.merge(f1,on=cats,how='left')\n",
    "\n",
    "    f2=df_train.groupby([\"VEHICLE_YEAR\",'INCOME']).target.agg(['mean','count']).reset_index()\n",
    "    f2.columns = ['VEHICLE_YEAR','INCOME'] + [''.join(col).strip()+ \"f2\" for col in f2.columns.values][2:]\n",
    "    df_train=df_train.merge(f2,on=['VEHICLE_YEAR','INCOME'],how='left')\n",
    "    df_test=df_test.merge(f2,on=['VEHICLE_YEAR','INCOME'],how='left')\n",
    "\n",
    "    f3=df_train.groupby([\"GENDER\",'TYPE_OF_VEHICLE']).target.agg(['mean','count']).reset_index()\n",
    "    f3.columns = [\"GENDER\",'TYPE_OF_VEHICLE'] + [''.join(col).strip()+ \"f3\" for col in f3.columns.values][2:]\n",
    "    df_train=df_train.merge(f3,on=[\"GENDER\",'TYPE_OF_VEHICLE'],how='left')\n",
    "    df_test=df_test.merge(f3,on=[\"GENDER\",'TYPE_OF_VEHICLE'],how='left')\n",
    "    \n",
    "    f4=df_train.groupby([\"AGE\",'EDUCATION','GENDER']).target.agg(['mean','count']).reset_index()\n",
    "    f4.columns = [\"AGE\",'EDUCATION','GENDER'] + [''.join(col).strip()+ \"f4\" for col in f4.columns.values][3:]\n",
    "    df_train=df_train.merge(f4,on=[\"AGE\",'EDUCATION','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f4,on=[\"AGE\",'EDUCATION','GENDER'],how='left')\n",
    "    \n",
    "    f5=df_train.groupby([\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER']).target.agg(['mean','count']).reset_index()\n",
    "    f5.columns = [\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'] + [''.join(col).strip()+ \"f5\" for col in f5.columns.values][3:]\n",
    "    df_train=df_train.merge(f5,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f5,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'],how='left')\n",
    "    \n",
    "#     f6=df_train.groupby([\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME']).target.agg(['mean','count']).reset_index()\n",
    "#     f6.columns = [\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'] + [''.join(col).strip()+ \"f6\" for col in f6.columns.values][3:]\n",
    "#     df_train=df_train.merge(f6,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'],how='left')\n",
    "#     df_test=df_test.merge(f6,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'],how='left')\n",
    "    \n",
    "    f7=df_train.groupby([\"AGE\",'DRIVING_EXPERIENCE','INCOME']).target.agg(['mean','count']).reset_index()\n",
    "    f7.columns = [\"AGE\",'DRIVING_EXPERIENCE','INCOME'] + [''.join(col).strip()+ \"f7\" for col in f7.columns.values][3:]\n",
    "    df_train=df_train.merge(f7,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME'],how='left')\n",
    "    df_test=df_test.merge(f7,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME'],how='left')\n",
    "    \n",
    "    f8=df_train.groupby([\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION']).target.agg(['mean','count']).reset_index()\n",
    "    f8.columns = [\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'] + [''.join(col).strip()+ \"f8\" for col in f8.columns.values][4:]\n",
    "    df_train=df_train.merge(f8,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'],how='left')\n",
    "    df_test=df_test.merge(f8,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'],how='left')\n",
    "    \n",
    "    \n",
    "    f9=df_train.groupby([\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER']).target.agg(['mean','count']).reset_index()\n",
    "    f9.columns = [\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'] + [''.join(col).strip()+ \"f9\" for col in f9.columns.values][4:]\n",
    "    df_train=df_train.merge(f9,on=[\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f9,on=[\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'],how='left')\n",
    "    \n",
    "    f10=df_train.groupby(['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS']).target.agg(['mean','count']).reset_index()\n",
    "    f10.columns = ['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'] + [''.join(col).strip()+ \"f10\" for col in f10.columns.values][3:]\n",
    "    df_train=df_train.merge(f10,on=['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'],how='left')\n",
    "    df_test=df_test.merge(f10,on=['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'],how='left')\n",
    "    \n",
    "    f11=df_train.groupby(['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE']).target.agg(['mean','count']).reset_index()\n",
    "    f11.columns = ['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'] + [''.join(col).strip()+ \"f11\" for col in f11.columns.values][3:]\n",
    "    df_train=df_train.merge(f11,on=['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'],how='left')\n",
    "    df_test=df_test.merge(f11,on=['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df_train,df_test\n",
    "\n",
    "# data=grouping(data)\n",
    "train,test=grouping(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4599,
   "id": "44537454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_2(df_train,df_test):\n",
    "    \n",
    "    cats=['TYPE_OF_VEHICLE','DRIVING_EXPERIENCE']\n",
    "    f1 = df_train[cats + ['target']].groupby(cats).agg(['var','sum',]).reset_index()\n",
    "    f1.columns = cats +['f1'+' '.join(col).strip() for col in f1.columns.values][2:]\n",
    "    df_train=df_train.merge(f1,on=cats,how='left')\n",
    "    df_test=df_test.merge(f1,on=cats,how='left')\n",
    "\n",
    "    f2=df_train.groupby([\"VEHICLE_YEAR\",'INCOME']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f2.columns = ['VEHICLE_YEAR','INCOME'] + [''.join(col).strip()+ \"f2\" for col in f2.columns.values][2:]\n",
    "    df_train=df_train.merge(f2,on=['VEHICLE_YEAR','INCOME'],how='left')\n",
    "    df_test=df_test.merge(f2,on=['VEHICLE_YEAR','INCOME'],how='left')\n",
    "\n",
    "    f3=df_train.groupby([\"GENDER\",'TYPE_OF_VEHICLE']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f3.columns = [\"GENDER\",'TYPE_OF_VEHICLE'] + [''.join(col).strip()+ \"f3\" for col in f3.columns.values][2:]\n",
    "    df_train=df_train.merge(f3,on=[\"GENDER\",'TYPE_OF_VEHICLE'],how='left')\n",
    "    df_test=df_test.merge(f3,on=[\"GENDER\",'TYPE_OF_VEHICLE'],how='left')\n",
    "    \n",
    "    f4=df_train.groupby([\"AGE\",'EDUCATION','GENDER']).target.agg(['var','sum',]).reset_index()\n",
    "    f4.columns = [\"AGE\",'EDUCATION','GENDER'] + [''.join(col).strip()+ \"f4\" for col in f4.columns.values][3:]\n",
    "    df_train=df_train.merge(f4,on=[\"AGE\",'EDUCATION','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f4,on=[\"AGE\",'EDUCATION','GENDER'],how='left')\n",
    "    \n",
    "    f5=df_train.groupby([\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f5.columns = [\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'] + [''.join(col).strip()+ \"f5\" for col in f5.columns.values][3:]\n",
    "    df_train=df_train.merge(f5,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f5,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','GENDER'],how='left')\n",
    "    \n",
    "#     f6=df_train.groupby([\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME']).target.agg(['mean','count']).reset_index()\n",
    "#     f6.columns = [\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'] + [''.join(col).strip()+ \"f6\" for col in f6.columns.values][3:]\n",
    "#     df_train=df_train.merge(f6,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'],how='left')\n",
    "#     df_test=df_test.merge(f6,on=[\"TYPE_OF_VEHICLE\",'EDUCATION','INCOME'],how='left')\n",
    "    \n",
    "    f7=df_train.groupby([\"AGE\",'DRIVING_EXPERIENCE','INCOME']).target.agg(['var','sum',]).reset_index()\n",
    "    f7.columns = [\"AGE\",'DRIVING_EXPERIENCE','INCOME'] + [''.join(col).strip()+ \"f7\" for col in f7.columns.values][3:]\n",
    "    df_train=df_train.merge(f7,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME'],how='left')\n",
    "    df_test=df_test.merge(f7,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME'],how='left')\n",
    "    \n",
    "    f8=df_train.groupby([\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION']).target.agg(['var','sum',]).reset_index()\n",
    "    f8.columns = [\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'] + [''.join(col).strip()+ \"f8\" for col in f8.columns.values][4:]\n",
    "    df_train=df_train.merge(f8,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'],how='left')\n",
    "    df_test=df_test.merge(f8,on=[\"AGE\",'DRIVING_EXPERIENCE','INCOME','EDUCATION'],how='left')\n",
    "    \n",
    "    \n",
    "    f9=df_train.groupby([\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f9.columns = [\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'] + [''.join(col).strip()+ \"f9\" for col in f9.columns.values][4:]\n",
    "    df_train=df_train.merge(f9,on=[\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'],how='left')\n",
    "    df_test=df_test.merge(f9,on=[\"INCOME\",'DRIVING_EXPERIENCE','TYPE_OF_VEHICLE','GENDER'],how='left')\n",
    "    \n",
    "    f10=df_train.groupby(['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f10.columns = ['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'] + [''.join(col).strip()+ \"f10\" for col in f10.columns.values][3:]\n",
    "    df_train=df_train.merge(f10,on=['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'],how='left')\n",
    "    df_test=df_test.merge(f10,on=['SPEEDING_VIOLATIONS','DUIS','PAST_ACCIDENTS'],how='left')\n",
    "    \n",
    "    f11=df_train.groupby(['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE']).target.agg([ 'var','sum',]).reset_index()\n",
    "    f11.columns = ['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'] + [''.join(col).strip()+ \"f11\" for col in f11.columns.values][3:]\n",
    "    df_train=df_train.merge(f11,on=['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'],how='left')\n",
    "    df_test=df_test.merge(f11,on=['SPEEDING_VIOLATIONS','PAST_ACCIDENTS','AGE'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df_train,df_test\n",
    "\n",
    "# data=grouping(data)\n",
    "train,test=grouping_2(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3613,
   "id": "f483fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(train, valid, col, target, kfold=5, smooth=20, gpu=False):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:   column which will be encoded (in the example RESOURCE)\n",
    "        target: target column which will be used to calculate the statistic\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume that the train dataset is shuffled\n",
    "    train['kfold'] = ((train.index) % kfold)\n",
    "    # We keep the original order as cudf merge will not preserve the original order\n",
    "    if gpu:\n",
    "        train['org_sorting'] = cupy.arange(len(train), dtype=\"int32\")\n",
    "    else:\n",
    "        train['org_sorting'] = np.arange(len(train), dtype=\"int32\")\n",
    "    # We create the output column, we fill with 0\n",
    "    col_name = '_'.join(col)\n",
    "    train['TE_' + col_name] = 0.\n",
    "    for i in range(kfold):\n",
    "        ###################################\n",
    "        # filter for out of fold\n",
    "        # calculate the mean/counts per group category\n",
    "        # calculate the global mean for the oof\n",
    "        # calculate the smoothed TE\n",
    "        # merge it to the original dataframe\n",
    "        ###################################\n",
    "        \n",
    "        df_tmp = train[train['kfold']!=i]\n",
    "        mn = df_tmp[target].mean()\n",
    "        df_tmp = df_tmp[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "        df_tmp.columns = col + ['mean', 'count']\n",
    "        df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "        df_tmp_m = train[col + ['kfold', 'org_sorting', 'TE_' + col_name]].merge(df_tmp, how='left', left_on=col, right_on=col).sort_values('org_sorting')\n",
    "        df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_' + col_name] = df_tmp_m.loc[df_tmp_m['kfold']==i, 'TE_tmp']\n",
    "        train['TE_' + col_name] = df_tmp_m['TE_' + col_name].fillna(mn).values\n",
    "\n",
    "    \n",
    "    ###################################\n",
    "    # calculate the mean/counts per group for the full training dataset\n",
    "    # calculate the global mean\n",
    "    # calculate the smoothed TE\n",
    "    # merge it to the original dataframe\n",
    "    # drop all temp columns\n",
    "    ###################################    \n",
    "    \n",
    "    df_tmp = train[col + [target]].groupby(col).agg(['mean', 'count']).reset_index()\n",
    "    mn = train[target].mean()\n",
    "    df_tmp.columns = col + ['mean', 'count']\n",
    "    df_tmp['TE_tmp'] = ((df_tmp['mean']*df_tmp['count'])+(mn*smooth)) / (df_tmp['count']+smooth)\n",
    "    if gpu:\n",
    "        valid['org_sorting'] = cupy.arange(len(valid), dtype=\"int32\")\n",
    "    else:\n",
    "        valid['org_sorting'] = np.arange(len(valid), dtype=\"int32\")\n",
    "    df_tmp_m = valid[col + ['org_sorting']].merge(df_tmp, how='left', left_on=col, right_on=col).sort_values('org_sorting')\n",
    "    valid['TE_' + col_name] = df_tmp_m['TE_tmp'].fillna(mn).values\n",
    "    \n",
    "    valid = valid.drop('org_sorting', axis=1)\n",
    "    train = train.drop('kfold', axis=1)\n",
    "    train = train.drop('org_sorting', axis=1)\n",
    "    return(train, valid)\n",
    "\n",
    "cats=[['TYPE_OF_VEHICLE'],['GENDER'],['AGE'],\n",
    "          ['EDUCATION'],['INCOME'],['GENDER','AGE','EDUCATION'],['TYPE_OF_VEHICLE','GENDER']]\n",
    "\n",
    "\n",
    "\n",
    "for cat in cats:\n",
    "    train, test = target_encode(train,test, cat, 'target')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3765,
   "id": "3b84dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encode(train, valid, col, gpu=False):\n",
    "    \"\"\"\n",
    "        train:  train dataset\n",
    "        valid:  validation dataset\n",
    "        col:    column which will be count encoded (in the example RESOURCE)\n",
    "    \"\"\"\n",
    "    # We keep the original order as cudf merge will not preserve the original order\n",
    "    if gpu:\n",
    "        train['org_sorting'] = cupy.arange(len(train), dtype=\"int32\")\n",
    "    else:\n",
    "        train['org_sorting'] = np.arange(len(train), dtype=\"int32\")\n",
    "    \n",
    "    train_tmp = train[col].value_counts().reset_index()\n",
    "    train_tmp.columns = [col,  'CE_' + col]\n",
    "    df_tmp = train[[col, 'org_sorting']].merge(train_tmp, how='left', left_on=col, right_on=col).sort_values('org_sorting')\n",
    "    train['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "        \n",
    "    if gpu:\n",
    "        valid['org_sorting'] = cupy.arange(len(valid), dtype=\"int32\")\n",
    "    else:\n",
    "        valid['org_sorting'] = np.arange(len(valid), dtype=\"int32\")\n",
    "    df_tmp = valid[[col, 'org_sorting']].merge(train_tmp, how='left', left_on=col, right_on=col).sort_values('org_sorting')\n",
    "    valid['CE_' + col] = df_tmp['CE_' + col].fillna(0).values\n",
    "    \n",
    "    valid = valid.drop('org_sorting', axis=1)\n",
    "    train = train.drop('org_sorting', axis=1)\n",
    "    return(train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4409,
   "id": "590062c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=[col for col in test.columns if test[col].dtype=='object']\n",
    "for cat in ['GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE']:\n",
    "    train, test = count_encode(train,test, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3856,
   "id": "29534dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_5(df): \n",
    "    \n",
    "    df['g1']=df.groupby(['EDUCATION','INCOME'])['CREDIT_SCORE'].transform('max')\n",
    "    df['g2']=df.groupby(['EDUCATION','INCOME'])['CREDIT_SCORE'].transform('mean')\n",
    "    df['g3']=df.groupby(['EDUCATION','INCOME'])['CREDIT_SCORE'].transform('min')\n",
    "    df['g4']=df.groupby(['EDUCATION','INCOME'])['CREDIT_SCORE'].transform('std')\n",
    "\n",
    "    df['g5']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['CREDIT_SCORE'].transform('max')\n",
    "    df['g6']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['CREDIT_SCORE'].transform('mean')\n",
    "    df['g7']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['CREDIT_SCORE'].transform('min')\n",
    "    df['g8']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['CREDIT_SCORE'].transform('std')\n",
    "\n",
    "    df['g9']=df.groupby(['EDUCATION','INCOME'])['ANNUAL_MILEAGE'].transform('max')\n",
    "    df['g10']=df.groupby(['EDUCATION','INCOME'])['ANNUAL_MILEAGE'].transform('mean')\n",
    "    df['g11']=df.groupby(['EDUCATION','INCOME'])['ANNUAL_MILEAGE'].transform('min')\n",
    "    df['g12']=df.groupby(['EDUCATION','INCOME'])['ANNUAL_MILEAGE'].transform('std')\n",
    "\n",
    "    df['g13']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['ANNUAL_MILEAGE'].transform('max')\n",
    "    df['g14']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['ANNUAL_MILEAGE'].transform('mean')\n",
    "    df['g15']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['ANNUAL_MILEAGE'].transform('min')\n",
    "    df['g16']=df.groupby(['DRIVING_EXPERIENCE','AGE'])['ANNUAL_MILEAGE'].transform('std')\n",
    "    return df\n",
    "train=feat_5(train)\n",
    "test=feat_5(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3741,
   "id": "cd4f815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_6(df):    \n",
    "    a=df.groupby(\"AGE\")['GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count' ,'nunique']).reset_index()\n",
    "    a.columns =  ['AGE']+['AGE' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "    b=df.groupby(\"DRIVING_EXPERIENCE\")['GENDER',\n",
    "     'AGE',\n",
    "     'EDUCATION',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count', 'nunique']).reset_index()\n",
    "    b.columns =  ['DRIVING_EXPERIENCE']+['DRIVING_EXPERIENCE' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "    c=df.groupby(\"GENDER\")['AGE',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count',  'nunique']).reset_index()\n",
    "    c.columns =  ['GENDER']+['GENDER' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "    d=df.groupby(\"EDUCATION\")['GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'AGE',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count',  'nunique']).reset_index()\n",
    "    d.columns =  ['EDUCATION']+['EDUCATION' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "    e=df.groupby(\"VEHICLE_YEAR\")['GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'AGE',\n",
    "     'INCOME',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count',  'nunique']).reset_index()\n",
    "    e.columns =  ['VEHICLE_YEAR']+['VEHICLE_YEAR' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "\n",
    "\n",
    "    g=df.groupby(\"INCOME\")['GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'AGE',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE'].agg(['count',  'nunique']).reset_index()\n",
    "    g.columns =  ['INCOME']+['INCOME' +'_'.join(x) for x in a.columns][1:]\n",
    "\n",
    "    df=df.merge(a,on='AGE').merge(b,on='DRIVING_EXPERIENCE').merge(c,on='GENDER').merge(d,on='EDUCATION').merge(e,on='VEHICLE_YEAR').merge(g,on='INCOME')\n",
    "    return df\n",
    "\n",
    "train=feat_6(train)\n",
    "test=feat_6(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3742,
   "id": "ebdda4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feat_7(df):    \n",
    "    for col in ['AGE',\n",
    "     'GENDER',\n",
    "     'DRIVING_EXPERIENCE',\n",
    "     'EDUCATION',\n",
    "     'INCOME',\n",
    "     'VEHICLE_YEAR',\n",
    "     'TYPE_OF_VEHICLE']:\n",
    "        a=df.groupby(col)['CREDIT_SCORE','ANNUAL_MILEAGE'].agg(['mean', 'std', 'min', 'max',]).reset_index()\n",
    "        a.columns =  [col]+[col +'_'.join(x) for x in a.columns][1:]\n",
    "        df=df.merge(a,on=col)\n",
    "    return df\n",
    "train=feat_7(train)\n",
    "test=feat_7(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4680,
   "id": "9e3edb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_8(df):  \n",
    "    for col in ['CREDIT_SCORE','ANNUAL_MILEAGE']:\n",
    "        df[col+'1']=1/df[col]\n",
    "        df[col+'2']=df[col]**2\n",
    "        df[col+'3']=df[col]**3\n",
    "    return df\n",
    "train=feat_8(train)\n",
    "test=feat_8(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4681,
   "id": "880a5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_9(df):    \n",
    "    col_1,col_2 ='CREDIT_SCORE','ANNUAL_MILEAGE'\n",
    "    \n",
    "    df[col+'1']=1/df[col]\n",
    "    df[col+'2']=df[col]**2\n",
    "    df[col+'3']=df[col]**3\n",
    "    df[col_1+col_2 +'1']=np.sqrt(df[col_1])/df[col_2]\n",
    "    df[col_1+col_2+'2']=1/(df[col_1]*df[col_2])\n",
    "    df[col_1+col_2+'3']=df[col_1]/df[col_2]\n",
    "    df[col_1+col_2+'4']=df[col_1]**3/df[col_2]\n",
    "    df[col_1+col_2+'5']=df[col_1]**2/df[col_2]\n",
    "#    df[col_1+col_2+'6']=np.sqrt(df[col_1])*np.sqrt(df[col_2])\n",
    "    df[col_1+col_2+'7']=np.sqrt(df[col_1])*df[col_2]**3\n",
    "    #df[col_1+col_2+'8']=df[col_1]**3*np.log(df[col_2])\n",
    "    df[col_1+col_2+'a']=df[col_1]**3/df[col_2]**3\n",
    "  #  df[col_1+col_2+'b']=np.sqrt(df[col_1]*df[col_2])\n",
    "    df[col_1+col_2+'c']=1/(df[col_1]+df[col_2])\n",
    "    df[col_1+col_2+'d']=1/(df[col_1]**3*df[col_2]**3)\n",
    "    return df\n",
    "train=feat_9(train)\n",
    "test=feat_9(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4673,
   "id": "b5fbecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp(data):    \n",
    "    feat1=data.groupby(\"CREDIT_SCORE\")[\"ANNUAL_MILEAGE\"].transform(\"mean\")\n",
    "    data['feat2']=data['CREDIT_SCORE'] - feat1\n",
    "\n",
    "\n",
    "    feat4=data.groupby(\"ANNUAL_MILEAGE\")[\"CREDIT_SCORE\"].transform(\"std\")\n",
    "    data['feat4']=data['ANNUAL_MILEAGE'] - feat4\n",
    "    \n",
    "    feat5=data.groupby(\"ANNUAL_MILEAGE\")[\"CREDIT_SCORE\"].transform(\"mean\")\n",
    "    data['feat5']=data['ANNUAL_MILEAGE'] - feat5\n",
    "    \n",
    "    return data\n",
    "train=grp(train)\n",
    "test=grp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4026bf32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorical encoding as there is order in some features, so doing Nominal encoding for these features\n",
    "def cat_encode(df):    \n",
    "    df['EDUCATION']=df['EDUCATION'].apply(lambda x: 0 if x=='university' else 1 if x=='high school' else 2) \n",
    "    df['DRIVING_EXPERIENCE']=df['DRIVING_EXPERIENCE'].apply(lambda x: 3 if x=='30y' else 2 if x=='20-29' else 1 if x=='10-19' else 0) \n",
    "    df['INCOME']=df['INCOME'].apply(lambda x: 3 if x=='upper class' else 2 if x=='middle class' else 1 if x=='working class' else 0)\n",
    "    \n",
    "#     df['edi']=df['EDUCATION']+df['DRIVING_EXPERIENCE']+df['INCOME']\n",
    "#     df['edi']=df['EDUCATION']*df['DRIVING_EXPERIENCE']*df['INCOME']\n",
    "    \n",
    "#     df['EDUCATION_rank']=df['EDUCATION'].rank()\n",
    "#     df['DRIVING_EXPERIENCE_rank']=df['DRIVING_EXPERIENCE'].rank()\n",
    "#     df['total_violations_rank']=df['total_violations'].rank()\n",
    "#     df['SPEEDING_VIOLATIONS_bi_rank']=df['SPEEDING_VIOLATIONS_bi'].rank()\n",
    "#     df['INCOME_rank']=df['INCOME'].rank()\n",
    "#     df['INCOME_rank']=df['INCOME'].rank()\n",
    "#    df['VEHICLE_YEAR']=df['VEHICLE_YEAR'].apply(lambda x: 1 if x=='after' else 0) \n",
    "    df['AGE']=df['AGE'].apply(lambda x: 0 if x=='65+' else 1 if x=='40-64' else 2 if x=='26-39' else 3)\n",
    "\n",
    "    return df\n",
    "\n",
    "# train=cat_encode(train)\n",
    "# test=cat_encode(test)\n",
    "del train['target']\n",
    "encoder=OrdinalEncoder()\n",
    "\n",
    "\n",
    "data=pd.concat((train,test)).reset_index(drop=True)\n",
    "# train=encoder.fit_transform(train,y)\n",
    "# test=encoder.transform(test)\n",
    "cat_cols=test.select_dtypes(include=['object'])\n",
    "\n",
    "# for other categorical features, doing simple numeric representation of feature by identifying distinct values.\n",
    "for col in cat_cols:\n",
    "    data[col],_ = data[col].factorize()\n",
    "\n",
    "train=data[:len(train)]\n",
    "test=data[len(train):]\n",
    "# train=pd.get_dummies(train)\n",
    "# test=pd.get_dummies(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b25c0862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9884585190307693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9884585190307693\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0009158680769923057, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0009158680769923057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810897843150918, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810897843150918\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0298690031084307, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0298690031084307\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "fold no ----->1\n",
      "training_log_loss-->>>0.6797734656278717\n",
      "val_log_loss-->>>0.6810534040834404\n",
      "*******************************************\n",
      "*******************************************\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9884585190307693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9884585190307693\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0009158680769923057, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0009158680769923057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810897843150918, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810897843150918\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0298690031084307, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0298690031084307\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "fold no ----->2\n",
      "training_log_loss-->>>0.6805940332553321\n",
      "val_log_loss-->>>0.6811309947141887\n",
      "*******************************************\n",
      "*******************************************\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9884585190307693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9884585190307693\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0009158680769923057, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0009158680769923057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810897843150918, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810897843150918\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0298690031084307, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0298690031084307\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "fold no ----->3\n",
      "training_log_loss-->>>0.6791060993883499\n",
      "val_log_loss-->>>0.6810512754228273\n",
      "*******************************************\n",
      "*******************************************\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9884585190307693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9884585190307693\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0009158680769923057, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0009158680769923057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810897843150918, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810897843150918\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0298690031084307, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0298690031084307\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "fold no ----->4\n",
      "training_log_loss-->>>0.6800565593175874\n",
      "val_log_loss-->>>0.6810845561545016\n",
      "*******************************************\n",
      "*******************************************\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9884585190307693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9884585190307693\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.0009158680769923057, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0009158680769923057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810897843150918, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810897843150918\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0298690031084307, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0298690031084307\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "fold no ----->5\n",
      "training_log_loss-->>>0.680617085259876\n",
      "val_log_loss-->>>0.6810614571791623\n",
      "*******************************************\n",
      "*******************************************\n",
      "train mean score  is ---->0.6800294485698034\n",
      "val mean score  is ---->0.681076337510824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#hyperparamater tunning done by optuna\n",
    "lgb_params= {'objective':'binary',\n",
    "        'metric':'binary_logloss',\n",
    "            'n_estimators':10000,\n",
    "             'learning_rate': 0.07268222670380756,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'lambda_l1': 0.0009158680769923057,\n",
    "    'lambda_l2':  0.0298690031084307,\n",
    "    'num_leaves': 109,\n",
    "    'feature_fraction':0.9884585190307693,\n",
    "    'bagging_fraction': 0.810897843150918,\n",
    "    'bagging_freq': 4,\n",
    "    'min_child_samples': 42,\n",
    "    'max_bin': 307} \n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier( **lgb_params)\n",
    "y_= np.log(y+1)\n",
    "\n",
    "kf=StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "lr_test_a = np.zeros((len(test),5))\n",
    "train_scores =[]\n",
    "val_score =[]\n",
    "out_of_fold = []\n",
    "\n",
    "\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(train, y)):\n",
    "    X_train,X_test,y_train,y_test= train[train_index] ,train[test_index],y[train_index], y[test_index]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     calibrated_clf.fit(X_train, y_train, verbose=False,\n",
    "#         eval_set=(X_test, y_test), early_stopping_rounds=500,callbacks=[lgb.log_evaluation(period=50, show_stdv=True)])\n",
    "    \n",
    "#     feature_importance_values += lgb_model.feature_importances_ / 5\n",
    "\n",
    "    lgb_model.fit(X_train, y_train, verbose=False,\n",
    "        eval_set=(X_test, y_test), early_stopping_rounds=500)\n",
    "    \n",
    "    pred_1=lgb_model.predict_proba(X_train,\n",
    "                             num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "    pred_2=lgb_model.predict_proba(X_test,\n",
    "                             num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    lr_test_a[:,fold_idx] = lgb_model.predict_proba(test,\n",
    "                             num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "    \n",
    "    \n",
    "\n",
    "   # loss_cliped=log_loss(y_test,np.clip(pred_1,0.055,0.975))\n",
    "    training_loss=log_loss(y_train,pred_1)\n",
    "    val_loss=log_loss(y_test,pred_2)\n",
    "    \n",
    "    #del pred_1,pred_2,lr_test_a\n",
    "    \n",
    "    print(f'fold no ----->{fold_idx+1}')\n",
    "    print(f'training_log_loss-->>>{training_loss}')\n",
    "    print(f'val_log_loss-->>>{val_loss}')\n",
    "          \n",
    "    print('*******************************************')\n",
    "    print('*******************************************')\n",
    "    \n",
    "  #  print(f'cliped_log_loss--->>{loss_cliped}')\n",
    "    train_scores.append(training_loss)\n",
    "    val_score.append(val_loss)\n",
    "    #oof_pred.append(oof)\n",
    "\n",
    "print(f'train mean score  is ---->{np.mean(train_scores)}')\n",
    "print(f'val mean score  is ---->{np.mean(val_score)}')  \n",
    "#feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4521,
   "id": "7070909b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train, y, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    \n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\":  trial.suggest_categorical(\"boosting_type\", [\"gbdt\",  \"dart\"]),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'max_bin':trial.suggest_int(\"max_bin\", 50, 800),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(valid_x)\n",
    "    loss = log_loss(valid_y, preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e1bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\",sampler=sampler,\n",
    "                    pruner=optuna.pruners.HyperbandPruner(min_resource=1, max_resource=n_train_iter, reduction_factor=3))\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ea7f8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=lr_test_a.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4708,
   "id": "78d5529a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"OUTCOME\":a})\n",
    "submission.to_csv('submission_1071.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4617a0c0ffe1381b5e09bb2a0f0022fd13d64e11533fbfad5706636ec3a1c06b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
